# =============================================================================
# DINOv2 ViT-L/14 encoding model â€” self-supervised visual features
# =============================================================================
# Uses Meta's DINOv2 ViT-L/14 (via timm) for feature extraction.
# DINOv2 learns visual features via self-supervised learning (no labels),
# producing representations that capture visual structure differently than
# supervised models like EmoNet or CLIP.
#
# To extract features:
#   amod-encoder extract-features -c configs/dinov2_amygdala.yaml \
#       --source /data/500_days_of_summer.mp4 \
#       --output output/dinov2_vit_l/movie_features.npy
#
# Then fit:
#   amod-encoder fit -c configs/dinov2_amygdala.yaml

paths:
  bids_root: /data/ds002837
  osf_fc7_mat: output/dinov2_vit_l/movie_features.npy
  output_dir: ./output/dinov2_vit_l/amygdala
  iaps_csv: /data/osf/IAPS_data_amygdala_z.csv
  oasis_csv: /data/osf/OASIS_data_amygdala_z.csv

subjects:
  - "01"
  - "02"
  - "03"
  - "04"
  - "05"
  - "06"
  - "07"
  - "08"
  - "09"
  - "10"
  - "11"
  - "12"
  - "13"
  - "14"
  - "15"
  - "16"
  - "17"
  - "18"
  - "19"
  - "20"

roi:
  - name: amygdala
    mask_path: /data/masks/canlab2018_amygdala_combined.nii.gz
    atlas: canlab2018

features:
  type: dinov2_vit_l
  n_features: 1024               # DINOv2 ViT-L output dim
  frame_sampling: 5
  align_method: resample_poly
  zscore: false
  convolution_order: resample_then_convolve

  extractor:
    backend: timm
    model_name: vit_large_patch14_dinov2.lvd142m
    layer: null
    pool: cls                     # DINOv2 uses CLS token as representation
    device: cuda

hrf:
  model: spm_canonical
  dt: 1.0

model:
  type: pls
  pls_n_components: 20
  standardize_X: true
  standardize_Y: true
  mode: voxelwise

cv:
  scheme: kfold
  n_folds: 5
  seed: 42
