# =============================================================================
# CLUSTER CONFIG — whole amygdala, Midway3 hcn1 node (4× L40S, 32 cores)
# Mirrors: replication_amygdala.yaml, optimized for full node utilization
#
# What changes from the local config:
#   compute.n_workers = 20   → all 20 subjects run in parallel (5 per GPU)
#   compute.gpu_ids   = [0,1,2,3]  → round-robin across 4× L40S (48 GB each)
#   compute.pin_memory = true       → faster host→device transfers
#   paths.*            → /project Lustre paths on Midway3
# =============================================================================

paths:
  bids_root:    /project/hcn1/dtfalk/Brain-Encoders/amod_encoder/data/NNDb_ds002837
  osf_fc7_mat:  /project/hcn1/dtfalk/Brain-Encoders/amod_encoder/data/features/500_days_of_summer_fc7_features.mat
  output_dir:   /project/hcn1/dtfalk/Brain-Encoders/results/amygdala
  iaps_csv:     /project/hcn1/dtfalk/Brain-Encoders/amod_encoder/data/ratings/IAPS_data_amygdala_z.csv
  oasis_csv:    /project/hcn1/dtfalk/Brain-Encoders/amod_encoder/data/ratings/OASIS_data_amygdala_z.csv

subjects:
  - "1"
  - "2"
  - "3"
  - "4"
  - "5"
  - "6"
  - "7"
  - "8"
  - "9"
  - "10"
  - "11"
  - "12"
  - "13"
  - "14"
  - "15"
  - "16"
  - "17"
  - "18"
  - "19"
  - "20"

roi:
  - name: amygdala
    mask_path: /project/hcn1/dtfalk/Brain-Encoders/amod_encoder/data/masks/canlab2018/canlab2018_amygdala_combined.nii.gz
    atlas: canlab2018

features:
  type: fc7
  n_features: 4096
  frame_sampling: 5
  align_method: resample_poly
  zscore: false
  convolution_order: resample_then_convolve

hrf:
  model: spm_canonical
  dt: 1.0

model:
  type: pls
  pls_n_components: 20
  standardize_X: false
  standardize_Y: false
  mode: voxelwise

cv:
  scheme: kfold
  n_folds: 5
  seed: 42

# ── L40S optimization ──────────────────────────────────────────────────────
# 20 workers × 4 GPUs = 5 subjects/GPU concurrently
# SIMPLS SVD runs on-device (torch.linalg.svd, ~4-8× faster than numpy)
# BOLD IO bottleneck: 1.5 GB/subject × 20 = 30 GB RAM  (well under 200 GB)
compute:
  backend: torch
  device: cuda        # overridden per-worker to cuda:0..3
  amp: false          # PLS is float64 arithmetic — AMP not applicable
  n_workers: 20       # joblib loky, one process per subject
  gpu_ids: [0, 1, 2, 3]
  pin_memory: true
