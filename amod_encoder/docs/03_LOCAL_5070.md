# Local Setup — NVIDIA RTX 5070 (Blackwell)

This guide covers setting up the Brain-Encoders pipeline on your local
workstation with an **NVIDIA GeForce RTX 5070** (Blackwell architecture,
sm_120, ~12 GB VRAM, CUDA 12.8).

> **Prerequisites:**
> - Complete the [Data Setup Guide](01_DATA_SETUP.md) first.
> - Read the [General Usage Guide](02_GENERAL_USAGE.md) for pipeline
>   commands and output structure.

---

## 1. Hardware Summary

| Component | Spec |
|-----------|------|
| GPU | NVIDIA GeForce RTX 5070 (Blackwell, ~12 GB VRAM) |
| Architecture | sm_120 |
| CUDA | 12.8 |
| PyTorch | Nightly (Blackwell not yet in stable release) |

---

## 2. Driver Setup

Make sure your NVIDIA drivers support CUDA 12.8+:

```powershell
nvidia-smi
```

You should see **Driver Version ≥ 570.x** and **CUDA Version: 12.8** (or
higher) in the output. If not, update your drivers from
<https://www.nvidia.com/drivers>.

---

## 3. Environment Setup

The repository includes a ready-made conda environment file tailored for
the RTX 5070.

### Create the Environment

```powershell
cd Brain-Encoders\amod_encoder

conda env create -f environment-local.yml
conda activate brain-encoders
```

This installs:
- **Python 3.11**
- **PyTorch nightly** with CUDA 12.8 (from `pytorch-nightly` channel +
  `--extra-index-url https://download.pytorch.org/whl/nightly/cu128`)
- **torchvision ≥ 0.22**
- **timm ≥ 1.0** and **transformers ≥ 4.40** (model-agnostic feature
  extraction)
- **opencv-python** (video frame extraction)
- All scientific and neuroimaging dependencies (NumPy, SciPy, nibabel,
  nilearn, statsmodels, etc.)

### Install the Package

After the conda env is active:

```powershell
pip install -e ".[all]"
```

The `[all]` extra includes GPU, extractor, and dev dependencies. Since
pytorch is already installed via conda, pip will see it as satisfied.

### Verify GPU Access

```powershell
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0)}')"
```

Expected output:

```
CUDA available: True
Device: NVIDIA GeForce RTX 5070
```

If CUDA is not available, check:
1. Driver version (`nvidia-smi`)
2. That you installed torch from the nightly/cu128 index
3. That `sm_120` support is included in your torch build:
   `python -c "import torch; print(torch.cuda.get_arch_list())"`

---

## 4. Configuration for GPU

To use the GPU for Ridge regression, update the `compute:` section in your
YAML config:

```yaml
compute:
  backend: torch
  device: cuda
  amp: false             # AMP (float16) optional; set true for faster Ridge
```

**PLS models always run on CPU** — the SIMPLS algorithm matches MATLAB's
`plsregress` and has no GPU path. This is by design for numerical fidelity.

**Ridge regression** benefits from GPU acceleration, especially with many
voxels. The `_fit_torch()` path uses PyTorch's `torch.linalg.solve` on the
GPU.

### AMP (Automatic Mixed Precision)

Setting `amp: true` enables float16 computation in Ridge's GPU path. This
roughly doubles throughput but introduces small numerical differences
(~1e-5). Leave it `false` for exact replication of paper results.

---

## 5. Running the Pipeline

All commands are the same as in the [General Usage Guide](02_GENERAL_USAGE.md).
The only difference is the `compute:` config section above.

### Whole Amygdala

```powershell
amod-encoder fit  --config configs/amod_amygdala.yaml
amod-encoder eval --config configs/amod_amygdala.yaml
amod-encoder validate --config configs/amod_amygdala.yaml
```

### Subregions

```powershell
amod-encoder fit  --config configs/amod_subregions.yaml
amod-encoder eval --config configs/amod_subregions.yaml
```

### Feature Extraction (Optional)

If you want to extract features from new images or video (instead of using
pre-computed fc7), the RTX 5070 makes this fast:

```powershell
# Extract CLIP features from IAPS images
amod-encoder extract-features \
  -c configs/clip_extractor.yaml \
  --source /path/to/IAPS/images \
  --output output/iaps_clip_features.npy \
  --batch-size 64

# Extract from movie video
amod-encoder extract-features \
  -c configs/dinov2_extractor.yaml \
  --source /path/to/500_days_of_summer.mp4 \
  --output output/movie_dinov2.npy \
  --batch-size 32
```

> **VRAM Note:** The RTX 5070 has ~12 GB VRAM. A batch size of 32–64 works
> well for most models. If you hit out-of-memory errors, reduce
> `--batch-size` to 16 or 8.

---

## 6. Performance Expectations

On the RTX 5070 with CPU backend (PLS):

| Step | Time per Subject |
|------|-----------------|
| `fit` (PLS, whole amygdala, CPU) | ~2–3 min |
| `fit` (PLS, 4 subregions, CPU) | ~8–12 min |
| `eval` (all subjects) | ~30 sec |
| `validate` | ~15 sec |
| `extract-features` (CLIP, 1000 images, GPU) | ~20 sec |

Ridge with `backend: torch` + `device: cuda` is ~5–10× faster than the
sklearn CPU path for large voxel counts.

---

## 7. Blackwell-Specific Notes

- **PyTorch nightly is required.** Blackwell (sm_120) is not yet supported
  in stable PyTorch releases. The `environment-local.yml` uses the
  `pytorch-nightly` conda channel and the nightly pip index. Once PyTorch
  stable adds sm_120 support, you can switch to the stable channel.

- **CUDA 12.8** — Blackwell requires CUDA 12.8 or higher. Earlier CUDA
  toolkits will not recognize the GPU.

- **Driver compatibility** — Use the latest Game Ready or Studio driver
  from NVIDIA (570.x+).

- If you encounter compilation errors from triton or torch extensions
  mentioning sm_120, ensure your torch nightly build is recent enough
  (check `python -c "import torch; print(torch.__version__)"`).

---

## 8. Falling Back to CPU

If GPU setup gives you trouble, the pipeline falls back gracefully. Set:

```yaml
compute:
  backend: cpu
  device: cpu
  amp: false
```

Everything works identically — just slower for Ridge. PLS is CPU-only
regardless. See the [General Usage Guide](02_GENERAL_USAGE.md) for full
CPU-only instructions.

---

## 9. Troubleshooting

| Problem | Solution |
|---------|----------|
| `torch.cuda.is_available()` returns `False` | Update NVIDIA driver to 570.x+; reinstall torch from nightly/cu128 |
| `RuntimeError: CUDA error: no kernel image is available for execution on the device` | Your torch build lacks sm_120. Install a newer nightly build. |
| `OutOfMemoryError` during `extract-features` | Reduce `--batch-size` (try 16 or 8) |
| `OutOfMemoryError` during Ridge `fit` | The 12 GB VRAM may be tight for very large ROIs. Fall back to CPU or enable AMP. |
| Conda solve takes forever | Add `--solver=libmamba` or use `mamba env create -f environment-local.yml` |
| `environment-local.yml` fails on non-NVIDIA machine | This env is 5070-specific. Use the [General Usage Guide](02_GENERAL_USAGE.md) pip install instead. |
